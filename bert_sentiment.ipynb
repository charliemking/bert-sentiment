{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngI9zOsBHaJJ"
      },
      "source": [
        "# Sentiment Analysis Fine-Tuning with BERT\n",
        "\n",
        "In this course project, I fine-tuned a pre-trained encoder-only language model called Bert (originally trained and released by Google in 2018) for a sentiment analysis task.\n\n Unlike a causal GPT-style language model, BERT is bidirectional in the sense that it was trained to predict a masked word in the middle of a sequence using both the previous and subsequent tokens. For example, BERT was trained on tasks like predicting the masked token in `The sweet black cat [MASK] by the window in the sun.` considering both the preceding tokens `The sweet black cat` **and** the subsequent tokens `by the window in the sun.`\n",
        "\n",
        "This kind of model is not used for autoregressively generating new text, but is very useful when you want to understand an entire sequence of text as a whole, allowing attention to earlier or later tokens in a sequence. Sentiment analysis, wherein we want to classify an entire input sequence as either positive or negative in sentiment (for example, in this text we classify movie reviews as either positive or negative), is a good example where this kind of understanding is important.\n",
        "\n",
        "In this part of the project, we will directly modify the `PyTorch` model and will conduct the fine-tuning directly in `PyTorch` as we have done with previous models.\n",
        "\n",
        "**Learning objectives.** You will:\n",
        "1. Examine an encoder-only BERT transformer model\n",
        "2. Modify a BERT model for sentiment analysis\n",
        "3. Fine-tune the model on movie review data for sentiment analysis\n",
        "\n",
        "While it is possible to complete this assignment using CPU compute, it may be slow. To accelerate your training, consider using GPU resources such as `CUDA` through the CS department cluster. Alternatives include Google colab or local GPU resources for those running on machines with GPU support."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "latwg1P5HaJK"
      },
      "source": [
        "First, ensure that you have the `transformers` and `datasets` modules installed. We will use these modules for importing tokenizers, pretrained models, and datasets. You can run the following cells to try to install them with `pip` if needed. If you are using ondemand, ideally you would simply include `module load transformers` and `module load datasets` when making your initial reservation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyATI_vmHaJK",
        "outputId": "92ed9459-f096-4fa9-e9ee-96ec10767c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1suozasHaJL",
        "outputId": "a3a29b6a-01b7-403d-d3fd-326d9b18a532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbXKgrUKHaJL"
      },
      "source": [
        "Now the following code imports a *tokenizer* and demonstrates its use.\n",
        "\n",
        "Note how the sequence of words in the input string is replaced with a sequence of numbers in the `input_ids`: These are indices into the vocabulary of 30522 used by the tokenizer. Also note the `special_tokens`: an `[UNK]` is used for anything not in the vocabulary, and a `[PAD]` can be useful for padding out a sequence of tokens to a specified length.\n",
        "\n",
        "Given a sequence of strings, the tokenizer returns a dictionary containing not just the `input_ids` (what you will most often want to use) but also `token_type_ids` (whether the token is special, which you will use least often) and `attention_mask`. The `attention_mask` has the same dimensions as the `input_ids` with a `1` in a given position if there is a non-padding token in that position and a `0` if that position is just a padding token. This is helpful when you are tokenizing a batch of multiple strings with potentially different lengths but want to create a single tensor. `padding='longest'` as shown pads all of the input to the same number of tokens as the longest input by adding `[PAD]` tokens to the end. The `attention_mask` is then passed so that you can ignore the extraneous padding tokens as needed.\n",
        "\n",
        "Also note the `return_tensors` parameter. Using `\"pt\"` as shown indicates that the results should be returned as PyTorch tensor. If you omit this parameter then the results will be returned as a Python list by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E_odVJ4HaJL",
        "outputId": "a425e572-29d9-4d44-a18f-434b4cce9464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertTokenizer(name_or_path='google-bert/bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
            "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n",
            ")\n",
            "{'input_ids': tensor([[  101,  1996, 11190,   102,     0,     0],\n",
            "        [  101,  5598,  2058,  1996,  4231,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "# run but you do not need to modify this code\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased',\n",
        "                                          clean_up_tokenization_spaces=True)\n",
        "print(tokenizer)\n",
        "tokenized = tokenizer([\"the cow\", \"jumped over the moon\"], padding='longest', return_tensors=\"pt\")\n",
        "print(tokenized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBeYMKpgHaJL"
      },
      "source": [
        "The tokenizer also has a `decode` method by which you can translate `input_ids` back into strings. You can optionally set `skip_special_tokens=True` if you want to ignore the special tokens like padding, unknown, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mATW4DhMHaJL",
        "outputId": "503cd19a-0112-4b43-f08c-5e0e578437da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the cow\n",
            "jumped over the moon\n"
          ]
        }
      ],
      "source": [
        "# run but you do not need to modify this code\n",
        "for tokens in tokenized[\"input_ids\"]:\n",
        "    print(tokenizer.decode(tokens, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATQCLohIHaJL"
      },
      "source": [
        "Now we import our language model, in this case a pretrained BERT model. This is an encoder-only transformer architecture previewed below. As you can see, the embedding expects a vocabulary of 30522 matching our tokenizer. The model embedding dimension is 768 and the output layer of the model also has 768 units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qTwTAd7HaJL",
        "outputId": "b8761915-76d9-4ebe-a18a-c748ee16b7b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertModel(\n",
            "  (embeddings): BertEmbeddings(\n",
            "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "    (position_embeddings): Embedding(512, 768)\n",
            "    (token_type_embeddings): Embedding(2, 768)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): BertEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0-11): 12 x BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSdpaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pooler): BertPooler(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (activation): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# run but you do not need to modify this code\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "pretrained_model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "print(pretrained_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll-xSgO4HaJL"
      },
      "source": [
        "## Task 1\n",
        "\n",
        "Our goal will be to modify a base Bert model for a sentiment analysis task. Specifically, we want to predict whether a given review text has a positive (1) or negative (0) sentiment. Define a model architecture that uses the pretrained BERT model but modifies it for classifying a sequence as positive or negative.\n",
        "\n",
        "Before proceeding, create a model object and ensure you can run forward progagation on a small example such as that defined in the second code block below. Your values may not be interpretable yet prior to fine-tuning, but you should be able to generate outputs of the correct shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cgoh-hsgHaJL"
      },
      "outputs": [],
      "source": [
        "# todo: define a model architecture for sentiment analysis using BERT\n",
        "class SentimentBert(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SentimentBert, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "        self.classifier = nn.Linear(768, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "        # todo: define model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ndhmBViHaJM",
        "outputId": "03c9a5bd-67b5-4856-a082-41adcbc5dd73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4364, -0.2077],\n",
            "        [-0.2758, -0.1643]], grad_fn=<AddmmBackward0>)\n",
            "torch.Size([2, 2])\n"
          ]
        }
      ],
      "source": [
        "# todo: try inference with your architecture here\n",
        "\n",
        "tokenized = tokenizer([\"the cow\", \"jumped over the moon\"], padding='longest', return_tensors=\"pt\")\n",
        "model = SentimentBert()\n",
        "logits = model(input_ids=tokenized[\"input_ids\"], attention_mask=tokenized[\"attention_mask\"])\n",
        "print(logits)\n",
        "print(logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTwdJRzpHaJM"
      },
      "source": [
        "## Task 2\n",
        "\n",
        "Our dataset is drawn from several thousand reviews on the Rotten Tomatoes website. Below we download and preview some of the data. Note that each element of a dataset is a dictionary with a `text` containing the review and a `label` which is `1` for a positive review or `0` for a negative review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ms-VjoGHaJM",
        "outputId": "0be5777e-ab70-437c-9c34-08548046bd05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training examples: 8530, Validation examples: 1066\n",
            "{'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .', 'label': 1}\n",
            "{'text': 'things really get weird , though not particularly scary : the movie is all portent and no content .', 'label': 0}\n",
            "{'text': 'effective but too-tepid biopic', 'label': 1}\n",
            "{'text': 'interminably bleak , to say nothing of boring .', 'label': 0}\n"
          ]
        }
      ],
      "source": [
        "# run but you do not need to modify this code\n",
        "from datasets import load_dataset\n",
        "train_data = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
        "val_data = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
        "\n",
        "print(f\"Training examples: {len(train_data)}, Validation examples: {len(val_data)}\")\n",
        "for i in range(1, 3):\n",
        "    print(train_data[i])\n",
        "    print(train_data[-i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5r-cHVcHaJM"
      },
      "source": [
        "As you can see, the reviews are not all the same length. It is better not to pad the entire dataset to the same length, and instead just to perform padding per batch. We will want to have `DataLoader`s for easy iteration over batches of data as tokenized tensors.\n",
        "\n",
        "One way to do this is to supply a `collate_fn` to the `DataLoader` constructor. This is a function that takes as input a list of elements from the dataset (called `batch`), which in our case will be a list of dictionaries containing `text` and `label` values. The function should return the batch with tokenized strings padded to the same length along with the corresponding values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMwci4hhHaJM"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate(batch):\n",
        "    tokenizer = BertTokenizer.from_pretrained('google-bert/bert-base-uncased')\n",
        "    texts = [item['text'] for item in batch]\n",
        "    labels = [item['label'] for item in batch]\n",
        "    tokenized = tokenizer(texts, padding='longest', return_tensors=\"pt\")\n",
        "    labels = torch.tensor(labels)\n",
        "    return {'input_ids': tokenized['input_ids'], 'attention_mask': tokenized['attention_mask'], 'labels': labels}\n",
        "    # todo: complete collate function\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate)\n",
        "val_dataloader = DataLoader(val_data, batch_size=8, shuffle=False, collate_fn=collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd-pASomHaJM",
        "outputId": "d052fa8c-c83a-4af1-d65c-f353683c417f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  1000,  1996,  4251,  2137,  1000,  4269,  1999, 24001,  1999,\n",
            "          3999,  1012,  2008,  1005,  1055,  2049,  2034,  3696,  1997,  4390,\n",
            "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  1037,  2143,  2008, 17567,  2138,  1997,  2049,  2116,  9987,\n",
            "          2229,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101, 17878,  2063, 15358,  2135,  1005,  1055, 17083,  1011,  4012,\n",
            "          3640, 25285,  1005,  1055, 14166, 21642,  7140,  2007,  2178,  6904,\n",
            "          8569,  2571,  5602,  4078,  7629,  1011,  1011,  1045,  1012,  1041,\n",
            "          1012,  1010,  1037,  7221,  2389,  6259,  8795,  1012,   102,     0],\n",
            "        [  101,  2009,  1005,  1055,  1037,  2843,  2000,  3198,  2111,  2000,\n",
            "          4133,  2145,  2005,  2048,  2847,  1998,  2689,  3666,  2107,  1037,\n",
            "          2839,  1010,  2926,  2043, 10155,  1999,  2004,  4257,  1998, 17727,\n",
            "         12054,  3512,  1037,  5450,  2004,  6708,  1005,  1055,  1012,   102],\n",
            "        [  101,  7266,  2039,  3110,  2066,  7167,  1997,  2060, 21864, 15952,\n",
            "          5691,  2008,  3046,  2000,  3556,  5099,  2791,  2685,  2007,  2402,\n",
            "          6001,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  1996,  6547,  1997, 26757,  2003,  2129,  1037,  4138,  3439,\n",
            "          3395,  1010,  4117,  2007,  2061,  2172,  2034,  1011,  3446,  5848,\n",
            "          1012,  1012,  1012,  2071,  2031, 17544,  2107,  1037,  4257,  1010,\n",
            "         20228,  7716,  4667,  3861,  1012,   102,     0,     0,     0,     0],\n",
            "        [  101, 25303, 29229,  8005,  2053, 17957,  1999,  2049, 15921,  1997,\n",
            "          1996,  3268,  1997,  1996,  6643,  8091,  2905,  1998,  1996,  2824,\n",
            "          2008,  2419,  2000,  2037, 12536,  4125,  2000,  1999,  7011,  8029,\n",
            "          1012,  1012,  1012,   102,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2028,  1997,  2216, 27547, 15693,  2008,  8145,  1037,  9129,\n",
            "          1997,  2111,  2040,  2024, 14727,  2055,  2242,  1998,  2059,  4481,\n",
            "          2041,  2129,  2000,  2191,  2149,  3745,  2037, 12024,  1012,   102,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([1, 0, 0, 0, 0, 0, 1, 1])}\n"
          ]
        }
      ],
      "source": [
        "# check if DataLoader is as intended\n",
        "for batch in train_dataloader:\n",
        "    print(batch)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPfZjEumHaJM"
      },
      "source": [
        "## Task 3\n",
        "\n",
        "Fine-tune the model on the training dataset until you achieve at least 80% accuracy on the validation dataset. You are welcome to use the [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) or [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer, whichever you prefer. As always, you may need to experiment to find a good learning rate or to decide on other optimization hyperparameters like momentum.\n",
        "\n",
        "You should track and evaluate the training loss at least every hundred batches. Evaluate the validation loss and accuracy at least once every epoch of training.\n",
        "\n",
        "Note that you are working with a relatively large model and should expect a single epoch to take several minutes, even using GPU compute. This is one reason we direct you to evaluate the training loss at least every hundred batches to monitor progress. With well-chosen hyperparameters, you should only need a small number (such as 1-3) epochs of fine-tuning; this should take minutes but not hours.\n",
        "\n",
        "Make sure to use the `attention_mask`, else the BERT model will be encoding unecessary `[PAD]` characters at the ends of sequences within a batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieH3rf80HaJM",
        "outputId": "f721dcbc-8d18-49a0-bf0b-3d00263d69fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 1, Loss: 0.5792637467384338\n",
            "Epoch 1, Batch 101, Loss: 0.6055983304977417\n",
            "Epoch 1, Batch 201, Loss: 0.3139762878417969\n",
            "Epoch 1, Batch 301, Loss: 0.3429502844810486\n",
            "Epoch 1, Batch 401, Loss: 0.1320042908191681\n",
            "Epoch 1, Batch 501, Loss: 0.8519556522369385\n",
            "Epoch 1, Batch 601, Loss: 0.2650899291038513\n",
            "Epoch 1, Batch 701, Loss: 0.18880772590637207\n",
            "Epoch 1, Batch 801, Loss: 0.4599440097808838\n",
            "Epoch 1, Batch 901, Loss: 0.13006779551506042\n",
            "Epoch 1, Batch 1001, Loss: 0.3385148048400879\n",
            "Epoch 1, Validation Loss: 0.38462158330413315, Validation Accuracy: 0.8611632270168855\n",
            "Validation accuracy reached 80%, stopping training.\n"
          ]
        }
      ],
      "source": [
        "# todo: fine-tune / train the modified BERT model for sentiment analysis\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SentimentBert().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1}, Batch {i+1}, Loss: {loss.item()}\")\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            total += labels.size(0)\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Epoch {epoch+1}, Validation Loss: {total_loss/len(train_dataloader)}, Validation Accuracy: {accuracy}\")\n",
        "    if accuracy >= 0.8:\n",
        "        print(\"Validation accuracy reached 80%, stopping training.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCvzUIyVHaJM"
      },
      "source": [
        "## Task 4\n",
        "\n",
        "Finally, retrieve five examples (your choice) from the validation dataset for which your fine-tuned model made incorrect predictions. Interpret the results on these five examples. Do you think the model is clearly incorrect or is there any ambiguity in whether the reviews are positive or negative?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3of7cKmHaJM",
        "outputId": "067724c1-412e-4618-cea8-d948b8b3418a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1:\n",
            "Text: made for teens and reviewed as such , this is recommended only for those under 20 years of age . . . and then only as a very mild rental .\n",
            "Label: 1\n",
            "Predicted Label: 0\n",
            "-----\n",
            "Example 2:\n",
            "Text: those moviegoers who would automatically bypass a hip-hop documentary should give \" scratch \" a second look .\n",
            "Label: 1\n",
            "Predicted Label: 0\n",
            "-----\n",
            "Example 3:\n",
            "Text: there's absolutely no reason why blue crush , a late-summer surfer girl entry , should be as entertaining as it is\n",
            "Label: 1\n",
            "Predicted Label: 0\n",
            "-----\n",
            "Example 4:\n",
            "Text: the events of the film are just so weird that i honestly never knew what the hell was coming next .\n",
            "Label: 1\n",
            "Predicted Label: 0\n",
            "-----\n",
            "Example 5:\n",
            "Text: full of bland hotels , highways , parking lots , with some glimpses of nature and family warmth , time out is a discreet moan of despair about entrapment in the maze of modern life .\n",
            "Label: 1\n",
            "Predicted Label: 0\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "# todo: code for task 4 here\n",
        "misclassified = []\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for example in val_data:\n",
        "        tokenized = tokenizer(example['text'], truncation=True, padding=True, return_tensors=\"pt\")\n",
        "        tokenized = {key: val.to(device) for key, val in tokenized.items()}\n",
        "        logits = model(input_ids=tokenized['input_ids'], attention_mask=tokenized['attention_mask'])\n",
        "        pred = torch.argmax(logits, dim=1).item()\n",
        "        if pred != example['label']:\n",
        "            misclassified.append({'text': example['text'], 'label': example['label'], 'pred': pred})\n",
        "        if len(misclassified) >= 5:\n",
        "            break\n",
        "\n",
        "for i, ex in enumerate(misclassified):\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(f\"Text: {ex['text']}\")\n",
        "    print(f\"Label: {ex['label']}\")\n",
        "    print(f\"Predicted Label: {ex['pred']}\")\n",
        "    print(\"-----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSo5M15THaJM"
      },
      "source": [
        "*briefly explain for task 4 here*\n",
        "\n",
        "The model clearly struggles with nuanced and sarcastic languge, but is not really messing up on clear examples. All these definitely have a sense of confusion or ambiguity involved which make it hard to correctly identify the best word. Alot of this is descriptive language like in \"full of bland hotels , highways , parking lots , with some glimpses of nature and family warmth , time out is a discreet moan of despair about entrapment in the maze of modern life \", not even I can tell exactly what energy this comment is trying to bring, let alone an AI model.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
